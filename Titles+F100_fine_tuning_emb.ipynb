{"cells":[{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1924,"status":"ok","timestamp":1740661499174,"user":{"displayName":"עמרי ליברטי","userId":"17630927866470282449"},"user_tz":-120},"id":"3h9dq7SIXGpt","outputId":"7ba31c8f-f86e-4923-9e07-35cd96c679ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"," agnostic_bert_article_embeddings.pkl\t datacluster_1_with_embeddings.pkl\n"," agnostic_bert_first100_embeddings.pkl\t datacluster_2_with_embeddings.pkl\n"," agnostic_bert_title_embeddings10k.pkl\t datacluster_3_with_embeddings.pkl\n"," agnostic_bert_title_embeddings.pkl\t datacluster_4_with_embeddings.pkl\n"," all-the-news-2-1.csv\t\t\t dbscan_Agnostic_clusters_analysis.csv\n"," bert_article_embeddings10K.pkl\t\t full_clusterd.pkl\n"," bert_article_embeddings.pkl\t\t full_clustered.pkl\n"," bert_first100_embeddings.pkl\t\t full_data_fixed_dates.pkl\n"," bert_title_embeddings10K.pkl\t\t full_dataset_Fine_embeddings.pkl\n"," cleaned_100k.csv\t\t\t full_dataset_with_embeddings.pkl\n"," cleaned_10k.csv\t\t\t full_data_with_dbscan.pkl\n"," cleaned_10k_with_first100.csv\t\t full_data_with_dbscan_revised.pkl\n"," cleaned_15k.csv\t\t\t'full_data_with_finetuned_embeddings&dbscan.pkl'\n"," cleaned_30k.csv\t\t\t full_data_with_finetuned_embeddings.pkl\n"," cleaned_full.csv\t\t\t full_data_with_finetuned_embeddings_titles.pkl\n"," cosine_titles_cluster_0.pkl\t\t full_data_with_hdbscan_subclusters.pkl\n"," cosine_titles_cluster_1.pkl\t\t full_data_with_optimized_dbscan.pkl\n"," cosine_titles_cluster_2.pkl\t\t full_embedding_15K.pkl\n"," cosine_titles_cluster_3.pkl\t\t hdbscan_parameter_analysis.csv\n"," cosine_titles_cluster_4.pkl\t\t last_rows_300000.csv\n"," cosine_titles_cluster_5.pkl\t\t last_rows_680004.csv\n"," datacluster_0_with_embeddings.pkl\t merged_cosine_titles.pkl\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import pandas as pd\n","from transformers import (\n","    BertTokenizer,\n","    BertForMaskedLM,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    TrainingArguments\n",")\n","import torch\n","\n","!ls \"/content/drive/My Drive/Colab_Notebooks/data\""]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":649},"executionInfo":{"elapsed":2043,"status":"ok","timestamp":1740661379728,"user":{"displayName":"עמרי ליברטי","userId":"17630927866470282449"},"user_tz":-120},"id":"dWwqtxrQXS0p","outputId":"f5eac24f-74fe-4a48-f40f-df12cb1746ab"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0.1  Unnamed: 0  year  month  day  \\\n","0       2489713     2489500  2019   11.0   29   \n","1       2491648     2491435  2019   12.0    2   \n","2       2491649     2491436  2019   12.0    2   \n","3       2491650     2491437  2019   12.0    2   \n","4       2491651     2491438  2019   12.0    2   \n","\n","                                              author  \\\n","0  Marcos Sainz, Returnly CTO & CNBC Technology E...   \n","1                                           Mark Hay   \n","2                                      Thomas Lewton   \n","3                                      David Gilbert   \n","4                                   Lil' Deb's Oasis   \n","\n","                                               title  \\\n","0  4 ways to take the headache and hassle out of ...   \n","1  Interview with a Couple Where One Partner Is H...   \n","2              Psychedelics Are for Queer Folks, Too   \n","3  Trump Won’t Take Part in This Week’s Landmark ...   \n","4                  Gluten Free Coconut Shrimp Recipe   \n","\n","                                             article  \\\n","0  Returns can be a monumental headache, not only...   \n","1  For many, HIV is the ultimate boogeyman of the...   \n","2   When Dee Adams decided to take an Ayahuasca t...   \n","3  President Donald Trump will not be attending t...   \n","4  Servings: 4Prep time: 15 minutesTotal time: 45...   \n","\n","                                                 url publication  \\\n","0  https://www.cnbc.com/2019/11/29/4-ways-to-take...        CNBC   \n","1  https://www.vice.com/en_us/article/pa7nw7/sero...        Vice   \n","2  https://www.vice.com/en_us/article/8xwz95/lgbt...        Vice   \n","3  https://www.vice.com/en_us/article/43kjqw/trum...        Vice   \n","4  https://www.vice.com/en_us/article/8xwpja/glut...        Vice   \n","\n","                                                F100  \\\n","0  Returns can be a monumental headache, not only...   \n","1  For many, HIV is the ultimate boogeyman of the...   \n","2  When Dee Adams decided to take an Ayahuasca tr...   \n","3  President Donald Trump will not be attending t...   \n","4  Servings: 4Prep time: 15 minutesTotal time: 45...   \n","\n","                                      bert_embedding  \\\n","0  [-0.45904645, -0.1318392, -0.13017993, 0.24272...   \n","1  [-0.11113172, -0.72078353, -1.2201693, 0.06096...   \n","2  [0.1243837, -0.023502616, -0.28594422, -0.2027...   \n","3  [-0.13597916, 0.026650878, 0.0071579902, 0.059...   \n","4  [-0.6875796, -0.013337972, -0.08837862, 0.0419...   \n","\n","                                 bert_F100_embedding  \\\n","0  [-0.45904645, -0.1318392, -0.13017993, 0.24272...   \n","1  [-0.11113172, -0.72078353, -1.2201693, 0.06096...   \n","2  [0.1243837, -0.023502616, -0.28594422, -0.2027...   \n","3  [-0.13597916, 0.026650878, 0.0071579902, 0.059...   \n","4  [-0.6875796, -0.013337972, -0.08837862, 0.0419...   \n","\n","                           agnostict_title_embedding  \\\n","0  [0.010146772, 0.045087833, 0.05110123, 0.02414...   \n","1  [-0.055526007, 0.13783266, -0.041953392, 0.003...   \n","2  [0.028439248, -0.029162535, 0.0124535, 0.02126...   \n","3  [0.04152333, 0.002287072, 0.0862064, -0.063820...   \n","4  [-0.05361938, 0.029068304, -0.052942466, 0.004...   \n","\n","                        agnostic_bert_F100_embedding  cluster  \\\n","0  [-0.06112704, -0.04745357, 0.0007734501, 0.001...        0   \n","1  [-0.015344072, 0.05901287, -0.07621075, 0.0554...        0   \n","2  [0.0025998014, 0.04433202, 0.015134116, 0.0711...        0   \n","3  [0.030706804, 0.0027535276, 0.05282167, -0.046...        2   \n","4  [-0.05276871, 0.014421557, 0.022626739, -0.002...        5   \n","\n","   ag_title_cluster  \n","0                 0  \n","1                 0  \n","2                 0  \n","3                 2  \n","4                 5  "],"text/html":["\n","  <div id=\"df-c2a8662a-71d1-484d-b6eb-2f36e437b242\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Unnamed: 0</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>author</th>\n","      <th>title</th>\n","      <th>article</th>\n","      <th>url</th>\n","      <th>publication</th>\n","      <th>F100</th>\n","      <th>bert_embedding</th>\n","      <th>bert_F100_embedding</th>\n","      <th>agnostict_title_embedding</th>\n","      <th>agnostic_bert_F100_embedding</th>\n","      <th>cluster</th>\n","      <th>ag_title_cluster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2489713</td>\n","      <td>2489500</td>\n","      <td>2019</td>\n","      <td>11.0</td>\n","      <td>29</td>\n","      <td>Marcos Sainz, Returnly CTO &amp; CNBC Technology E...</td>\n","      <td>4 ways to take the headache and hassle out of ...</td>\n","      <td>Returns can be a monumental headache, not only...</td>\n","      <td>https://www.cnbc.com/2019/11/29/4-ways-to-take...</td>\n","      <td>CNBC</td>\n","      <td>Returns can be a monumental headache, not only...</td>\n","      <td>[-0.45904645, -0.1318392, -0.13017993, 0.24272...</td>\n","      <td>[-0.45904645, -0.1318392, -0.13017993, 0.24272...</td>\n","      <td>[0.010146772, 0.045087833, 0.05110123, 0.02414...</td>\n","      <td>[-0.06112704, -0.04745357, 0.0007734501, 0.001...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2491648</td>\n","      <td>2491435</td>\n","      <td>2019</td>\n","      <td>12.0</td>\n","      <td>2</td>\n","      <td>Mark Hay</td>\n","      <td>Interview with a Couple Where One Partner Is H...</td>\n","      <td>For many, HIV is the ultimate boogeyman of the...</td>\n","      <td>https://www.vice.com/en_us/article/pa7nw7/sero...</td>\n","      <td>Vice</td>\n","      <td>For many, HIV is the ultimate boogeyman of the...</td>\n","      <td>[-0.11113172, -0.72078353, -1.2201693, 0.06096...</td>\n","      <td>[-0.11113172, -0.72078353, -1.2201693, 0.06096...</td>\n","      <td>[-0.055526007, 0.13783266, -0.041953392, 0.003...</td>\n","      <td>[-0.015344072, 0.05901287, -0.07621075, 0.0554...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2491649</td>\n","      <td>2491436</td>\n","      <td>2019</td>\n","      <td>12.0</td>\n","      <td>2</td>\n","      <td>Thomas Lewton</td>\n","      <td>Psychedelics Are for Queer Folks, Too</td>\n","      <td>When Dee Adams decided to take an Ayahuasca t...</td>\n","      <td>https://www.vice.com/en_us/article/8xwz95/lgbt...</td>\n","      <td>Vice</td>\n","      <td>When Dee Adams decided to take an Ayahuasca tr...</td>\n","      <td>[0.1243837, -0.023502616, -0.28594422, -0.2027...</td>\n","      <td>[0.1243837, -0.023502616, -0.28594422, -0.2027...</td>\n","      <td>[0.028439248, -0.029162535, 0.0124535, 0.02126...</td>\n","      <td>[0.0025998014, 0.04433202, 0.015134116, 0.0711...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2491650</td>\n","      <td>2491437</td>\n","      <td>2019</td>\n","      <td>12.0</td>\n","      <td>2</td>\n","      <td>David Gilbert</td>\n","      <td>Trump Won’t Take Part in This Week’s Landmark ...</td>\n","      <td>President Donald Trump will not be attending t...</td>\n","      <td>https://www.vice.com/en_us/article/43kjqw/trum...</td>\n","      <td>Vice</td>\n","      <td>President Donald Trump will not be attending t...</td>\n","      <td>[-0.13597916, 0.026650878, 0.0071579902, 0.059...</td>\n","      <td>[-0.13597916, 0.026650878, 0.0071579902, 0.059...</td>\n","      <td>[0.04152333, 0.002287072, 0.0862064, -0.063820...</td>\n","      <td>[0.030706804, 0.0027535276, 0.05282167, -0.046...</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2491651</td>\n","      <td>2491438</td>\n","      <td>2019</td>\n","      <td>12.0</td>\n","      <td>2</td>\n","      <td>Lil' Deb's Oasis</td>\n","      <td>Gluten Free Coconut Shrimp Recipe</td>\n","      <td>Servings: 4Prep time: 15 minutesTotal time: 45...</td>\n","      <td>https://www.vice.com/en_us/article/8xwpja/glut...</td>\n","      <td>Vice</td>\n","      <td>Servings: 4Prep time: 15 minutesTotal time: 45...</td>\n","      <td>[-0.6875796, -0.013337972, -0.08837862, 0.0419...</td>\n","      <td>[-0.6875796, -0.013337972, -0.08837862, 0.0419...</td>\n","      <td>[-0.05361938, 0.029068304, -0.052942466, 0.004...</td>\n","      <td>[-0.05276871, 0.014421557, 0.022626739, -0.002...</td>\n","      <td>5</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2a8662a-71d1-484d-b6eb-2f36e437b242')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c2a8662a-71d1-484d-b6eb-2f36e437b242 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c2a8662a-71d1-484d-b6eb-2f36e437b242');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a96c73c6-0ae6-41b7-a71d-02b04d2fcd5d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a96c73c6-0ae6-41b7-a71d-02b04d2fcd5d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a96c73c6-0ae6-41b7-a71d-02b04d2fcd5d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 15000,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0.1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17000,\n        \"min\": 2489713,\n        \"max\": 2548008,\n        \"num_unique_values\": 15000,\n        \"samples\": [\n          2534069,\n          2506790,\n          2536352\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17000,\n        \"min\": 2489500,\n        \"max\": 2547795,\n        \"num_unique_values\": 15000,\n        \"samples\": [\n          2533856,\n          2506577,\n          2536139\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2017,\n        \"max\": 2020,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2017,\n          2020,\n          2019\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3937509906732284,\n        \"min\": 1.0,\n        \"max\": 12.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          12.0,\n          9.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          25,\n          13,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4000,\n        \"samples\": [\n          \"Matt Spetalnick, Patricia Zengerle, David Brunnstrom\",\n          \"By Ralph Ellis and Chuck Johnston, CNN\",\n          \"William James, Kylie MacLellan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12960,\n        \"samples\": [\n          \"Texans snap Titans' streak to seize AFC South lead\",\n          \"Ficken's last-play FG lifts Jets over Dolphins\",\n          \"Brandon Tierney Says Competitors \\u2018Fool Themselves\\u2019 Into Thinking They Can Prep for 'Man Vs Bear'\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13019,\n        \"samples\": [\n          \"A woman who appeared on Spain\\u2019s version of the reality show Big Brother in 2017 says that she was sexually assaulted while unconscious \\u2014 and that producers had her watch the footage the next day without forewarning her. According to the allegations, first published earlier this week by Spain-based digital newspaper El Confidencial, Carlota Prado, 26, had developed a romantic relationship during filming with her alleged attacker, Jos\\u00e9 Mar\\u00eda L\\u00f3pez. (PEOPLE does not normally identify sexual assault victims, but Prado has publicly spoken out about the alleged attack.) L\\u00f3pez, also 26, has denied forcing himself on Prado in late November 2017 after a night of heavy drinking inside the house, the BBC reports. The New York Times reports an investigating judge has decided to pursue a criminal case, though no indictments had been issued as of Thursday. Both were contestants on the 18th season of Gran Hermano, and shared breakfast together the following morning. When discussing the previous evening, L\\u00f3pez told Prado she was inebriated and that he took care of her, making sure she got to bed. L\\u00f3pez was dismissed from the competition the day after the alleged sexual assault occurred. The International Business Times reports Prado says she was summoned to the show\\u2019s \\u201cdiary room\\u201d \\u2014 where a contestant is interviewed on camera by someone not in the room \\u2014 the next day, where producers showed her footage of the alleged attack with no forewarning. Prado says she pleaded repeatedly that the disturbing footage be halted. Yet, the tape continued to roll, causing her to weep. In the afternoon, Prado was told that L\\u00f3pez had been banished from the house. She says her repeated requests to talk to him about the incident were denied. The footage never aired. It was, however, obtained by El Confidencial, which described it in detail. Want to keep up with the latest crime coverage? Sign up for PEOPLE\\u2019s free True Crime newsletter for breaking crime news, ongoing trial coverage and details of intriguing unsolved cases. The video allegedly shows L\\u00f3pez helping a highly intoxicated Prado into the bed they shared that night. The outlet describes that, in the footage, L\\u00f3pez is seen trying to initiate sex with Prado, who rebuffs his advance, saying, \\u201cNo, I can\\u2019t.\\u201d Prado then passes out, at which point L\\u00f3pez allegedly forces intercourse with her. PEOPLE was unable to reach L\\u00f3pez or a rep for him. A statement in Spanish posted to the Twitter account of the broadcaster, Mediaset Espana, said a contestant had been expelled for \\u201cintolerable\\u201d behavior that had been reported to authorities. Prado spoke to El Confidencial, expressing outrage over how producers handled the situation. \\u201cThey allowed him to stay by my side many hours when they had sufficient proof to get him out immediately and then decide what to do with him,\\u201d she said, noting the producers let L\\u00f3pez \\u201claugh at my face\\u201d the next morning \\u201cby telling me that he looked after me.\\u201d She added: \\u201cI cannot understand how the program allowed this.\\u201d\",\n          \"SURKHET, Nepal (Thomson Reuters Foundation) - If it wasn\\u2019t for her self-annointed \\u201cBig Sister\\u201d, Punam Pun Magar would have quit school at 14 to marry a man nearly twice her age, bear him babies and tend house. Now she\\u2019s hoping to become a lawyer. Two in five Nepalese girls just like Magar marry before they turn 18: one of the highest rates in the world, despite child marriage being illegal in the impoverished Himalayan country. The bad times for Magar began when both her parents died and her aunt\\u2019s family felt burdened, saying she must pay her way. \\u201cThey told me not to go to school and do household chores. After a point, they wanted to get rid of me ... so they started planning my marriage to a 26-year-old man,\\u201d Magar said as tears welled up. That\\u2019s when Big Sister Krishna Paudel rode to her rescue, snatching her from a potential life of illiteracy, poverty and ill health: common fallout of so many child marriages in Nepal. Hundreds of Big Sisters - many of them former child brides themselves - have volunteered to counsel teenaged girls like Magar, as well as their families and communities, on the impact of marrying young, using their own stories as cautionary tales. \\u201cWhen I met her, I told her about child marriage - the legal consequences, the social fallout, everything. Since then, I\\u2019ve seen phenomenal change in her. That knowledge empowered her and now she\\u2019s headed towards a bright future,\\u201d said Paudel. The legal age of marriage in Nepal is 20 for men and women alike. Yet child marriage remains deeply rooted in conservative, mainly Hindu Nepal, where many parents marry off their teenaged daughters to boost the wider family finances. This drives a vicious cycle of ill health, malnutrition and ignorance, since a child bride is more likely to leave school and experience problems in pregnancy or birth, say campaigners. Some also face domestic and sexual abuse. Nepal has the third highest child marriage prevalence in South Asia, according to the United Nations. The \\u2018Sisters for Sisters\\u2019 program was introduced as part of a government drive to end child marriages in Nepal by 2030. The Sisters\\u2019 top job - to keep girls in school. Activists say dropout rates rise when girls are co-opted into household chores, pushed into early marriage or held back by discrimination and deep-seated taboos over periods. So when 25-year-old Paudel noticed Magar\\u2019s attendance dipping, she tracked her down and weighed in with advice - and kept dishing out the same message over many months. The wedding was called off; Magar went back to school. \\u201cHad it not been for my Big Sister, I would have had three or four children by now, and they would be studying here instead of me,\\u201d said Magar, 17, at her government-run school in western Nepal\\u2019s Surkhet district. A wife at 15 then a mother at 17, Big Sister Rachana Bantha said she had grappled with poor health and poverty since her forced marriage a decade ago. \\u201cI felt like killing myself. I remember how horrible it all was - but that is what motivates me every day to help these girls. They should not have to go through what I did,\\u201d Bantha told the Thomson Reuters Foundation. \\u201cI can save their lives,\\u201d she said, wearing the Big Sisters\\u2019 uniform of pink tunic and black pleated trousers. Bantha said she had stopped at least a dozen child marriages in the past four years. But the crime remains widespread, said Khagendra Bahadur Ruchal, an administrative official at Surkhet district. He blamed poverty and illiteracy, as well as parents hoping to stop the taboo of unmarried sex and pregnancy. The main enemy, however, is custom. \\u201cIt is ingrained in our society\\u2019s fabric. It is considered the norm. Even politicians and teachers are marrying their children off in some places. If they don\\u2019t practice what they preach, how can we expect any change?\\u201d he said. Teenagers are also eloping more often, a trend campaigners attribute to better access to mobile phones and the internet. As for the cause, they said some girls are fleeing poverty or forced marriage, others chase independence and sexual freedom. Ruchal the official said it was important to normalize live-in relationships so teenagers did not feel compelled to marry. Nepal should also give girls some sort of incentive to stay in school so they aspire to a career of their own, said Sumnima Tuladhar of the Kathmandu-based child rights group CWIN Nepal. \\u201cThey need to be excited about education. They don\\u2019t see a future after finishing school. We have to create a society where young people have something more than marriage to look forward to,\\u201d she said. Poverty is the main problem with girls routinely pushed into domestic work in a country where one in five survives on less than $1.25 a day, said Ananda Paudel of development charity VSO, which is behind the Sisters for Sister project. The program began in 2017 and has boosted girls\\u2019 confidence along with their school attendance, said Paudel. \\u201cThey are so empowered. Had it not been for this, we cannot imagine where they would have been right now. How many would have disappeared from the system, the society,\\u201d he said. And the results already show. Magar - alert in her blue school uniform - seems worlds away from the 14-year-old orphan who came so close to dropping out. \\u201cI\\u2019m going to study to become a lawyer so that I can help women. They face so much discrimination and do not find legal assistance,\\u201d she said. \\u201cI want to help them give a voice.\\u201d Reporting by Annie Banerji @anniebanerji, Additional reporting by Gopal Sharma in Kathmandu, Editing by Lyndsay Griffiths; Please credit the Thomson Reuters Foundation, the charitable arm of Thomson Reuters that covers humanitarian issues, conflicts, land and property rights, modern slavery and human trafficking, gender equality, climate change and resilience. Visit http://news.trust.org to see more stories\",\n          \"Welcome, baby Viola! When Calls the Heart\\u00a0star Andrea Brooks has revealed her baby girl\\u2019s name, speaking with\\u00a0Entertainment Tonight\\u00a0on Monday \\u2014 two days after giving birth to Viola, her first child \\u2014 about its touching inspiration. \\u201cIt\\u2019s a family name that belonged to my great-grandmother,\\u201d said Brooks, 30. \\u201cI have one of her lockets from the early 1900s and \\u2018Viola\\u2019 is inscribed on the back.\\u201d \\u201cWhen my husband and I started dating, I put a photo of the two of us in the locket,\\u201d the new mom added. \\u201cThe name has always meant a lot to me.\\u201d Never miss a story \\u2014 sign up for PEOPLE\\u2019s free daily newsletter to stay up-to-date on the best of what PEOPLE has to offer, from juicy celebrity news to compelling human interest stories. Brooks announced the arrival of her baby girl on Saturday, sharing a black-and-white image of herself cradling her newborn daughter. In the cute image, little Viola snuggled close to her mama\\u2019s chest as the mother-daughter duo shared a knit blanket. Brooks looked up at the camera, while her new addition appeared to be fast asleep. \\u201d \\u2026 and then there was you. \\ud83d\\udc95,\\u201d the Supergirl actress\\u00a0captioned the sweet announcement post on Instagram. Friends and fans were quick to flood the comments with congratulations. Her Hallmark costar Pascale Hutton left a red heart emoji under the post, while Riverdale\\u00a0actress Vanessa Morgan commented, \\u201cCrying. Can\\u2019t stop crying.\\u201d Brooks first announced her pregnancy with her baby girl in July, sharing with ET that she was five months along at the time. In the weeks prior to welcoming Viola, she told the outlet that she was prepping for the arrival by avoiding Google so as to not scare herself before the birth. Last month, Brooks shared a peek inside her nursery once it was finally complete. In two posts\\u00a0on Instagram, the star revealed she decided to go against a traditional pink room for her daughter, opting for a cool blue theme. She also told\\u00a0ET\\u00a0that she wanted the nursery to be a \\u201ccalming oasis\\u201d for her daughter, and its theme was inspired by the actress\\u2019s time in living in Vancouver, Canada.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15000,\n        \"samples\": [\n          \"https://www.cnn.com/2019/12/03/health/blue-zones-diet-food-wellness/index.html\",\n          \"https://www.reuters.com/article/japan-stocks-close/nikkei-hits-14-month-high-value-stocks-jump-on-trade-deal-bets-idUSL4N28N1U9\",\n          \"https://www.cnn.com/2019/12/19/media/fox-and-friends-impeachment/index.html\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"publication\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"CNN\",\n          \"Vice\",\n          \"New Yorker\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F100\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13005,\n        \"samples\": [\n          \"Only the best deals on Verge-approved gadgets get the Good Deals stamp of approval, so if you're looking for a deal on your next gadget or gift from major retailers like Amazon, Walmart, Best Buy, Target, and more, this is the place to be. Vox Media has affiliate partnerships. These do not influence editorial content, though Vox Media may earn commissions for products purchased via affiliate links. For more information, see our ethics policy. We\\u2019re less than a week away from Christmas, but deals are still popping up all over the web. Whether you\\u2019re doing last-minute holiday shopping or just\",\n          \"New York (CNN Business)From the beginning of the 737 Max crisis, Boeing (BA) CEO Dennis Muilenburg severely underestimated how much trouble the company faced. On Monday, that cost him his job. Boeing's problems with the 737 Max led to two fatal crashes, the loss of 346 lives, and costs that are expected to add up to more than $10 billion according to multiple analysts. It will take years for Boeing to recover from the problems the plane caused. Muilenburg was stripped of his Boeing chairmanship in October, but the company still expressed confidence in his abilities. Last month, David Calhoun\",\n          \"We got our first look at a PlayStation 5 game tonight during The Game Awards. It\\u2019s called Godfall, and it's a Destiny 2-looking loot shooter developed by Counterplay Games. The trailer is light on details, but we know that Godfall will be a third-person action RPG with loot, dungeons, and three-player coop. Godfall and the PlayStation 5 both launch during the \\\"holidays\\\" in 2020. Details about the new system are scarce, but we do know it\\u2019ll use a solid state hard drive to cut down on loading times and resolutions up to 8K. The controller will use haptic feedback, similar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_F100_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"agnostict_title_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"agnostic_bert_F100_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cluster\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ag_title_cluster\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":16}],"source":["df=pd.read_pickle('/content/drive/My Drive/Colab_Notebooks/data/full_clustered.pkl')\n","df.head()"]},{"cell_type":"code","source":[],"metadata":{"id":"1OfBPhX2PGll"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3738,"status":"ok","timestamp":1740651959486,"user":{"displayName":"עמרי ליברטי","userId":"17630927866470282449"},"user_tz":-120},"id":"owDvuyHVED95","outputId":"5af2d130-47c0-4edf-e0fe-2ed66fc19b7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Collecting datasets\n","  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip install transformers datasets"]},{"cell_type":"markdown","source":["#Fine tuning training function"],"metadata":{"id":"O5zdM-Cwzedw"}},{"cell_type":"code","source":["import pandas as pd\n","from transformers import (\n","    BertTokenizer,\n","    BertForMaskedLM,\n","    BertModel,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    TrainingArguments\n",")\n","import torch\n","import numpy as np\n","from datasets import Dataset\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","\n","# נתיב בסיסי לשמירת המודלים\n","model_base_path = \"/content/drive/My Drive/Colab_Notebooks/models/\"\n","\n","# פונקציה להפקת embedding מהמודל שעבר fine-tuning\n","def get_finetuned_embedding(text, model, tokenizer):\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n","    return embedding\n","\n","# הפונקציה המשולבת לאימון ולשמירת embeddings\n","def fine_tune_cluster(df, cluster_num, epochs=3):\n","    print(f\"\\n🚀 Fine-Tuning Cluster #{cluster_num}\")\n","\n","    # סינון הדאטה לפי קלאסטר ספציפי\n","    df_cluster = df[df['ag_title_cluster'] == cluster_num].copy()\n","\n","    # יצירת רשימת טקסטים לאימון\n","    texts = df_cluster['F100'].tolist()\n","\n","    # יצירת Dataset של HuggingFace\n","    dataset = Dataset.from_dict({\"text\": texts})\n","\n","    # פיצול הדאטה לסט אימון (90%) וסט הערכה (10%)\n","    split_dataset = dataset.train_test_split(test_size=0.1)\n","    train_dataset = split_dataset['train']\n","    eval_dataset = split_dataset['test']\n","\n","    # טעינת המודל והטוקנייזר של BERT\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","\n","    # פונקציה להמרת הטקסט לטוקנים\n","    def tokenize_function(example):\n","        return tokenizer(\n","            example['text'],\n","            padding='max_length',\n","            truncation=True,\n","            max_length=128\n","        )\n","\n","    # המרת הדאטה לטוקנים (גם ל-train וגם ל-eval)\n","    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n","    tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n","\n","    # יצירת DataCollator לאימון Masked Language Modeling\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=True,\n","        mlm_probability=0.15\n","    )\n","\n","    # הגדרת הפרמטרים של האימון\n","    training_args = TrainingArguments(\n","        output_dir=f\"{model_base_path}/fine_tuned_bert_cluster_{cluster_num}\",\n","        evaluation_strategy=\"epoch\",\n","        learning_rate=2e-5,\n","        num_train_epochs=epochs,\n","        weight_decay=0.01,\n","        per_device_train_batch_size=8,\n","        save_total_limit=2,\n","        logging_steps=50\n","    )\n","\n","    # יצירת האובייקט Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_eval,\n","        data_collator=data_collator\n","    )\n","\n","    # הרצת האימון\n","    trainer.train()\n","\n","    # שמירת המודל לאחר האימון בתיקייה המתאימה\n","    model_save_path = f\"{model_base_path}/fine_tuned_bert_cluster_{cluster_num}\"\n","    os.makedirs(model_save_path, exist_ok=True)  # יצירת התיקייה אם לא קיימת\n","\n","    trainer.save_model(model_save_path)\n","    tokenizer.save_pretrained(model_save_path)\n","\n","    print(f\"\\n✅ Cluster #{cluster_num} Fine-Tuned and Model Saved at {model_save_path}\")\n","\n","    # הפקת embeddings חדשים לכל הטקסטים בקלאסטר הנוכחי עם המודל החדש\n","    fine_tuned_model = BertModel.from_pretrained(model_save_path)\n","\n","    embeddings_list = []\n","    for idx, text in enumerate(df_cluster['F100']):\n","        embedding = get_finetuned_embedding(text, fine_tuned_model, tokenizer)\n","        embeddings_list.append(embedding)\n","        if (idx + 1) % 100 == 0 or (idx + 1) == len(df_cluster):\n","            print(f\"Processed {idx+1}/{len(df_cluster)} embeddings.\")\n","\n","    # שמירת ה-embeddings לעמודה החדשה\n","    df_cluster['F100_finetuning_embedding'] = embeddings_list\n","\n","    # החזרת ה-DataFrame המעודכן\n","    return df_cluster\n","\n","# טעינת הדאטה המקורי\n","df = pd.read_pickle('/content/drive/My Drive/Colab_Notebooks/data/full_data_with_finetuned_embeddings.pkl')\n","\n","# יצירת DataFrame אחד גדול מכל הקלאסטרים\n","all_clusters_dfs = []\n","\n","for cluster_num in df['ag_title_cluster'].unique():\n","    cluster_df = fine_tune_cluster(df, cluster_num, epochs=3)\n","    all_clusters_dfs.append(cluster_df)\n","\n","# איחוד כל הקלאסטרים ל-DataFrame אחד\n","final_df = pd.concat(all_clusters_dfs, ignore_index=True)\n","\n","# שמירת הקובץ המאוחד ל-Drive שלך\n","final_df.to_pickle('/content/drive/My Drive/Colab_Notebooks/data/Full_data_with_finetuned_embeddings.pkl')\n","\n","print(\"\\n🎉 All clusters fine-tuned and embeddings saved successfully!\")\n","print(\"✅ Data saved at: /content/drive/My Drive/Colab_Notebooks/data/Full_data_with_finetuned_embeddings.pkl\")"],"metadata":{"id":"p4_mw37_yvPo","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"status":"error","timestamp":1740661817396,"user_tz":-120,"elapsed":32486,"user":{"displayName":"עמרי ליברטי","userId":"17630927866470282449"}},"outputId":"3d651448-0df4-4066-d609-7e9c289ab33d"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-ead105d7e35a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# טעינת הדאטה המקורי\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab_Notebooks/data/full_data_with_finetuned_embeddings.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# יצירת DataFrame אחד גדול מכל הקלאסטרים\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m_frombuffer\u001b[0;34m(buf, dtype, shape, order)\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1851\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_frombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1852\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import BertTokenizer, BertModel\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Base path for your fine-tuned models\n","model_base_path = \"/content/drive/My Drive/Colab_Notebooks/models/\"\n","\n","# Load the DataFrame containing your data and cluster assignments\n","df = pd.read_pickle('/content/drive/My Drive/Colab_Notebooks/data/full_clustered.pkl')\n","\n","\n","# Check if all values in 'F100' are strings\n","if not all(isinstance(x, str) for x in df['F100']):\n","    # Delete rows where 'F100' is not a string\n","    df = df[df['F100'].apply(lambda x: isinstance(x, str))]\n","\n","\n","\n","# Function to generate embeddings using a fine-tuned model\n","def get_finetuned_embedding(text, model, tokenizer):\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n","    return embedding\n","\n","# Iterate through clusters and generate embeddings\n","all_clusters_dfs = []\n","for cluster_num in df['ag_title_cluster'].unique():\n","    print(f\"\\n🚀 Generating Embeddings for Cluster #{cluster_num}\")\n","\n","    # Filter data for the current cluster\n","    df_cluster = df[df['ag_title_cluster'] == cluster_num].copy()\n","\n","    # Load the fine-tuned model and tokenizer for this cluster\n","    model_path = os.path.join(model_base_path, f\"fine_tuned_bert_cluster_{cluster_num}\")\n","    tokenizer = BertTokenizer.from_pretrained(model_path)\n","    model = BertModel.from_pretrained(model_path)\n","\n","    # Generate embeddings for 'F100' texts in the cluster\n","    embeddings_list = []\n","    for idx, text in enumerate(df_cluster['F100']):\n","        embedding = get_finetuned_embedding(text, model, tokenizer)\n","        embeddings_list.append(embedding)\n","        if (idx + 1) % 100 == 0 or (idx + 1) == len(df_cluster):\n","            print(f\"Processed {idx+1}/{len(df_cluster)} embeddings.\")\n","\n","    # Add the embeddings to the DataFrame\n","    df_cluster['F100_finetuning_embedding'] = embeddings_list\n","    all_clusters_dfs.append(df_cluster)\n","\n","# Combine all cluster DataFrames\n","final_df = pd.concat(all_clusters_dfs, ignore_index=True)\n","\n","# Save the DataFrame with the embeddings\n","final_df.to_pickle('/content/drive/My Drive/Colab_Notebooks/data/Full_data_with_finetuned_embeddings.pkl')\n","print(\"\\n🎉 All F100 embeddings generated and saved successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JiWN8n8Vh5Y0","executionInfo":{"status":"ok","timestamp":1740668158341,"user_tz":-120,"elapsed":4549540,"user":{"displayName":"עמרי ליברטי","userId":"17630927866470282449"}},"outputId":"ea4db4ca-b246-4d5a-a84c-19b2706cc648"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","🚀 Generating Embeddings for Cluster #0\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_0 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/3818 embeddings.\n","Processed 200/3818 embeddings.\n","Processed 300/3818 embeddings.\n","Processed 400/3818 embeddings.\n","Processed 500/3818 embeddings.\n","Processed 600/3818 embeddings.\n","Processed 700/3818 embeddings.\n","Processed 800/3818 embeddings.\n","Processed 900/3818 embeddings.\n","Processed 1000/3818 embeddings.\n","Processed 1100/3818 embeddings.\n","Processed 1200/3818 embeddings.\n","Processed 1300/3818 embeddings.\n","Processed 1400/3818 embeddings.\n","Processed 1500/3818 embeddings.\n","Processed 1600/3818 embeddings.\n","Processed 1700/3818 embeddings.\n","Processed 1800/3818 embeddings.\n","Processed 1900/3818 embeddings.\n","Processed 2000/3818 embeddings.\n","Processed 2100/3818 embeddings.\n","Processed 2200/3818 embeddings.\n","Processed 2300/3818 embeddings.\n","Processed 2400/3818 embeddings.\n","Processed 2500/3818 embeddings.\n","Processed 2600/3818 embeddings.\n","Processed 2700/3818 embeddings.\n","Processed 2800/3818 embeddings.\n","Processed 2900/3818 embeddings.\n","Processed 3000/3818 embeddings.\n","Processed 3100/3818 embeddings.\n","Processed 3200/3818 embeddings.\n","Processed 3300/3818 embeddings.\n","Processed 3400/3818 embeddings.\n","Processed 3500/3818 embeddings.\n","Processed 3600/3818 embeddings.\n","Processed 3700/3818 embeddings.\n","Processed 3800/3818 embeddings.\n","Processed 3818/3818 embeddings.\n","\n","🚀 Generating Embeddings for Cluster #2\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_2 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/1686 embeddings.\n","Processed 200/1686 embeddings.\n","Processed 300/1686 embeddings.\n","Processed 400/1686 embeddings.\n","Processed 500/1686 embeddings.\n","Processed 600/1686 embeddings.\n","Processed 700/1686 embeddings.\n","Processed 800/1686 embeddings.\n","Processed 900/1686 embeddings.\n","Processed 1000/1686 embeddings.\n","Processed 1100/1686 embeddings.\n","Processed 1200/1686 embeddings.\n","Processed 1300/1686 embeddings.\n","Processed 1400/1686 embeddings.\n","Processed 1500/1686 embeddings.\n","Processed 1600/1686 embeddings.\n","Processed 1686/1686 embeddings.\n","\n","🚀 Generating Embeddings for Cluster #5\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_5 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/2388 embeddings.\n","Processed 200/2388 embeddings.\n","Processed 300/2388 embeddings.\n","Processed 400/2388 embeddings.\n","Processed 500/2388 embeddings.\n","Processed 600/2388 embeddings.\n","Processed 700/2388 embeddings.\n","Processed 800/2388 embeddings.\n","Processed 900/2388 embeddings.\n","Processed 1000/2388 embeddings.\n","Processed 1100/2388 embeddings.\n","Processed 1200/2388 embeddings.\n","Processed 1300/2388 embeddings.\n","Processed 1400/2388 embeddings.\n","Processed 1500/2388 embeddings.\n","Processed 1600/2388 embeddings.\n","Processed 1700/2388 embeddings.\n","Processed 1800/2388 embeddings.\n","Processed 1900/2388 embeddings.\n","Processed 2000/2388 embeddings.\n","Processed 2100/2388 embeddings.\n","Processed 2200/2388 embeddings.\n","Processed 2300/2388 embeddings.\n","Processed 2388/2388 embeddings.\n","\n","🚀 Generating Embeddings for Cluster #1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_1 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/2575 embeddings.\n","Processed 200/2575 embeddings.\n","Processed 300/2575 embeddings.\n","Processed 400/2575 embeddings.\n","Processed 500/2575 embeddings.\n","Processed 600/2575 embeddings.\n","Processed 700/2575 embeddings.\n","Processed 800/2575 embeddings.\n","Processed 900/2575 embeddings.\n","Processed 1000/2575 embeddings.\n","Processed 1100/2575 embeddings.\n","Processed 1200/2575 embeddings.\n","Processed 1300/2575 embeddings.\n","Processed 1400/2575 embeddings.\n","Processed 1500/2575 embeddings.\n","Processed 1600/2575 embeddings.\n","Processed 1700/2575 embeddings.\n","Processed 1800/2575 embeddings.\n","Processed 1900/2575 embeddings.\n","Processed 2000/2575 embeddings.\n","Processed 2100/2575 embeddings.\n","Processed 2200/2575 embeddings.\n","Processed 2300/2575 embeddings.\n","Processed 2400/2575 embeddings.\n","Processed 2500/2575 embeddings.\n","Processed 2575/2575 embeddings.\n","\n","🚀 Generating Embeddings for Cluster #3\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_3 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/1196 embeddings.\n","Processed 200/1196 embeddings.\n","Processed 300/1196 embeddings.\n","Processed 400/1196 embeddings.\n","Processed 500/1196 embeddings.\n","Processed 600/1196 embeddings.\n","Processed 700/1196 embeddings.\n","Processed 800/1196 embeddings.\n","Processed 900/1196 embeddings.\n","Processed 1000/1196 embeddings.\n","Processed 1100/1196 embeddings.\n","Processed 1196/1196 embeddings.\n","\n","🚀 Generating Embeddings for Cluster #4\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_4 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/3335 embeddings.\n","Processed 200/3335 embeddings.\n","Processed 300/3335 embeddings.\n","Processed 400/3335 embeddings.\n","Processed 500/3335 embeddings.\n","Processed 600/3335 embeddings.\n","Processed 700/3335 embeddings.\n","Processed 800/3335 embeddings.\n","Processed 900/3335 embeddings.\n","Processed 1000/3335 embeddings.\n","Processed 1100/3335 embeddings.\n","Processed 1200/3335 embeddings.\n","Processed 1300/3335 embeddings.\n","Processed 1400/3335 embeddings.\n","Processed 1500/3335 embeddings.\n","Processed 1600/3335 embeddings.\n","Processed 1700/3335 embeddings.\n","Processed 1800/3335 embeddings.\n","Processed 1900/3335 embeddings.\n","Processed 2000/3335 embeddings.\n","Processed 2100/3335 embeddings.\n","Processed 2200/3335 embeddings.\n","Processed 2300/3335 embeddings.\n","Processed 2400/3335 embeddings.\n","Processed 2500/3335 embeddings.\n","Processed 2600/3335 embeddings.\n","Processed 2700/3335 embeddings.\n","Processed 2800/3335 embeddings.\n","Processed 2900/3335 embeddings.\n","Processed 3000/3335 embeddings.\n","Processed 3100/3335 embeddings.\n","Processed 3200/3335 embeddings.\n","Processed 3300/3335 embeddings.\n","Processed 3335/3335 embeddings.\n","\n","🎉 All F100 embeddings generated and saved successfully!\n"]}]},{"cell_type":"code","source":["# prompt: now normalize It l2\n","import numpy as np\n","from sklearn.preprocessing import normalize\n","\n","# Assuming 'final_df' is your DataFrame and 'F100_finetuning_embedding' is the column containing the embeddings\n","# Convert the embeddings to numpy arrays for normalization\n","embeddings_np = np.array(final_df['F100_finetuning_embedding'].tolist())\n","\n","# Normalize the embeddings using L2 norm\n","normalized_embeddings = normalize(embeddings_np, norm='l2')\n","\n","# Convert the normalized embeddings back to a list of lists\n","normalized_embeddings_list = normalized_embeddings.tolist()\n","\n","# Update the DataFrame with the normalized embeddings\n","final_df['F100_finetuning_embedding_normalized'] = normalized_embeddings_list\n","\n","# Save the updated DataFrame\n","final_df.to_pickle('/content/drive/My Drive/Colab_Notebooks/data/New_Normalized_Finetuned.pkl')\n","print(\"\\n🎉 Embeddings normalized and saved successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgCujk3bicEF","executionInfo":{"status":"ok","timestamp":1740669031307,"user_tz":-120,"elapsed":11318,"user":{"displayName":"עמרי ליברטי","userId":"17630927866470282449"}},"outputId":"ff2b43b4-bb9c-42e0-92bc-21dc9b1f6394"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🎉 Embeddings normalized and saved successfully!\n"]}]},{"cell_type":"markdown","source":["##Fine Tuning emb on titles"],"metadata":{"id":"DoPGOzWNmu1w"}},{"cell_type":"code","source":["##modle on titles\n","import pandas as pd\n","import torch\n","import numpy as np\n","from transformers import BertTokenizer, BertModel\n","from google.colab import drive\n","import os\n","from sklearn.preprocessing import normalize\n","\n","drive.mount('/content/drive')\n","\n","# נתיב לתיקיית המודלים המאומנים\n","model_base_path = \"/content/drive/My Drive/Colab_Notebooks/models/\"\n","\n","# טעינת הנתונים (שכבר כוללים את ה-F100 embeddings)\n","df = pd.read_pickle('/content/drive/My Drive/Colab_Notebooks/data/full_data_with_finetuned_embeddings.pkl')\n","\n","# פונקציה להפקת embedding מהמודל שעבר fine-tuning\n","def get_finetuned_embedding(text, model, tokenizer):\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n","    return embedding\n","\n","# מעבר על כל קלאסטר והפקת embeddings לכותרות\n","all_clusters_dfs = []\n","\n","for cluster_num in df['ag_title_cluster'].unique():\n","    print(f\"\\n🚀 Extracting Title Embeddings for Cluster #{cluster_num}\")\n","\n","    # סינון הנתונים של הקלאסטר\n","    cluster_df = df[df['ag_title_cluster'] == cluster_num].copy()\n","\n","    # בדיקת קיום המודל המאומן\n","    model_path = os.path.join(model_base_path, f\"fine_tuned_bert_cluster_{cluster_num}\")\n","\n","    if not os.path.exists(model_path):\n","        print(f\"🚨 Model for Cluster {cluster_num} not found at {model_path}! Skipping...\")\n","        continue\n","\n","    # טעינת המודל והטוקנייזר שעברו אימון עבור הקלאסטר הזה\n","    tokenizer = BertTokenizer.from_pretrained(model_path)\n","    model = BertModel.from_pretrained(model_path)\n","\n","    # יצירת רשימת embeddings\n","    embeddings_list = []\n","    for idx, text in enumerate(cluster_df['title']):\n","        embedding = get_finetuned_embedding(text, model, tokenizer)\n","        embeddings_list.append(embedding)\n","        if (idx + 1) % 100 == 0 or (idx + 1) == len(cluster_df):\n","            print(f\"Processed {idx+1}/{len(cluster_df)} title embeddings.\")\n","\n","    # שמירת ה-embeddings בעמודה חדשה\n","    cluster_df['finetuning_title_embedding'] = embeddings_list\n","\n","    # הוספה לרשימת הקלאסטרים\n","    all_clusters_dfs.append(cluster_df)\n","\n","# איחוד כל הקלאסטרים ל-DataFrame אחד\n","final_df = pd.concat(all_clusters_dfs, ignore_index=True)\n","\n","# Normalize the embeddings\n","final_df['normalized_titles_finetuned_embedding'] = df['finetuning_embedding'].apply(lambda x: normalize(x.reshape(1, -1), norm='l2').flatten())\n","\n","# Save the updated DataFrame\n","final_df.to_pickle('/content/drive/My Drive/Colab_Notebooks/data/full_data_with_finetuned_embeddings.pkl')\n","print(\"\\n🎉 All title Normalembeddings extracted successfully!\")\n","print(\"✅ Data saved at: /content/drive/My Drive/Colab_Notebooks/data/full_data_with_finetuned_embeddings.pkl\")"],"metadata":{"id":"zY4gdETS9vbn","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1740566512235,"user_tz":-120,"elapsed":1725173,"user":{"displayName":"עמרי ליברטי","userId":"17630927866470282449"}},"outputId":"4926b676-ff26-47ab-c0b7-937f1d5a4676"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","🚀 Extracting Title Embeddings for Cluster #0\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_0 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/3818 title embeddings.\n","Processed 200/3818 title embeddings.\n","Processed 300/3818 title embeddings.\n","Processed 400/3818 title embeddings.\n","Processed 500/3818 title embeddings.\n","Processed 600/3818 title embeddings.\n","Processed 700/3818 title embeddings.\n","Processed 800/3818 title embeddings.\n","Processed 900/3818 title embeddings.\n","Processed 1000/3818 title embeddings.\n","Processed 1100/3818 title embeddings.\n","Processed 1200/3818 title embeddings.\n","Processed 1300/3818 title embeddings.\n","Processed 1400/3818 title embeddings.\n","Processed 1500/3818 title embeddings.\n","Processed 1600/3818 title embeddings.\n","Processed 1700/3818 title embeddings.\n","Processed 1800/3818 title embeddings.\n","Processed 1900/3818 title embeddings.\n","Processed 2000/3818 title embeddings.\n","Processed 2100/3818 title embeddings.\n","Processed 2200/3818 title embeddings.\n","Processed 2300/3818 title embeddings.\n","Processed 2400/3818 title embeddings.\n","Processed 2500/3818 title embeddings.\n","Processed 2600/3818 title embeddings.\n","Processed 2700/3818 title embeddings.\n","Processed 2800/3818 title embeddings.\n","Processed 2900/3818 title embeddings.\n","Processed 3000/3818 title embeddings.\n","Processed 3100/3818 title embeddings.\n","Processed 3200/3818 title embeddings.\n","Processed 3300/3818 title embeddings.\n","Processed 3400/3818 title embeddings.\n","Processed 3500/3818 title embeddings.\n","Processed 3600/3818 title embeddings.\n","Processed 3700/3818 title embeddings.\n","Processed 3800/3818 title embeddings.\n","Processed 3818/3818 title embeddings.\n","\n","🚀 Extracting Title Embeddings for Cluster #1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_1 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/2575 title embeddings.\n","Processed 200/2575 title embeddings.\n","Processed 300/2575 title embeddings.\n","Processed 400/2575 title embeddings.\n","Processed 500/2575 title embeddings.\n","Processed 600/2575 title embeddings.\n","Processed 700/2575 title embeddings.\n","Processed 800/2575 title embeddings.\n","Processed 900/2575 title embeddings.\n","Processed 1000/2575 title embeddings.\n","Processed 1100/2575 title embeddings.\n","Processed 1200/2575 title embeddings.\n","Processed 1300/2575 title embeddings.\n","Processed 1400/2575 title embeddings.\n","Processed 1500/2575 title embeddings.\n","Processed 1600/2575 title embeddings.\n","Processed 1700/2575 title embeddings.\n","Processed 1800/2575 title embeddings.\n","Processed 1900/2575 title embeddings.\n","Processed 2000/2575 title embeddings.\n","Processed 2100/2575 title embeddings.\n","Processed 2200/2575 title embeddings.\n","Processed 2300/2575 title embeddings.\n","Processed 2400/2575 title embeddings.\n","Processed 2500/2575 title embeddings.\n","Processed 2575/2575 title embeddings.\n","\n","🚀 Extracting Title Embeddings for Cluster #2\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_2 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/1686 title embeddings.\n","Processed 200/1686 title embeddings.\n","Processed 300/1686 title embeddings.\n","Processed 400/1686 title embeddings.\n","Processed 500/1686 title embeddings.\n","Processed 600/1686 title embeddings.\n","Processed 700/1686 title embeddings.\n","Processed 800/1686 title embeddings.\n","Processed 900/1686 title embeddings.\n","Processed 1000/1686 title embeddings.\n","Processed 1100/1686 title embeddings.\n","Processed 1200/1686 title embeddings.\n","Processed 1300/1686 title embeddings.\n","Processed 1400/1686 title embeddings.\n","Processed 1500/1686 title embeddings.\n","Processed 1600/1686 title embeddings.\n","Processed 1686/1686 title embeddings.\n","\n","🚀 Extracting Title Embeddings for Cluster #3\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_3 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/1196 title embeddings.\n","Processed 200/1196 title embeddings.\n","Processed 300/1196 title embeddings.\n","Processed 400/1196 title embeddings.\n","Processed 500/1196 title embeddings.\n","Processed 600/1196 title embeddings.\n","Processed 700/1196 title embeddings.\n","Processed 800/1196 title embeddings.\n","Processed 900/1196 title embeddings.\n","Processed 1000/1196 title embeddings.\n","Processed 1100/1196 title embeddings.\n","Processed 1196/1196 title embeddings.\n","\n","🚀 Extracting Title Embeddings for Cluster #4\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_4 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/3335 title embeddings.\n","Processed 200/3335 title embeddings.\n","Processed 300/3335 title embeddings.\n","Processed 400/3335 title embeddings.\n","Processed 500/3335 title embeddings.\n","Processed 600/3335 title embeddings.\n","Processed 700/3335 title embeddings.\n","Processed 800/3335 title embeddings.\n","Processed 900/3335 title embeddings.\n","Processed 1000/3335 title embeddings.\n","Processed 1100/3335 title embeddings.\n","Processed 1200/3335 title embeddings.\n","Processed 1300/3335 title embeddings.\n","Processed 1400/3335 title embeddings.\n","Processed 1500/3335 title embeddings.\n","Processed 1600/3335 title embeddings.\n","Processed 1700/3335 title embeddings.\n","Processed 1800/3335 title embeddings.\n","Processed 1900/3335 title embeddings.\n","Processed 2000/3335 title embeddings.\n","Processed 2100/3335 title embeddings.\n","Processed 2200/3335 title embeddings.\n","Processed 2300/3335 title embeddings.\n","Processed 2400/3335 title embeddings.\n","Processed 2500/3335 title embeddings.\n","Processed 2600/3335 title embeddings.\n","Processed 2700/3335 title embeddings.\n","Processed 2800/3335 title embeddings.\n","Processed 2900/3335 title embeddings.\n","Processed 3000/3335 title embeddings.\n","Processed 3100/3335 title embeddings.\n","Processed 3200/3335 title embeddings.\n","Processed 3300/3335 title embeddings.\n","Processed 3335/3335 title embeddings.\n","\n","🚀 Extracting Title Embeddings for Cluster #5\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/Colab_Notebooks/models/fine_tuned_bert_cluster_5 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Processed 100/2388 title embeddings.\n","Processed 200/2388 title embeddings.\n","Processed 300/2388 title embeddings.\n","Processed 400/2388 title embeddings.\n","Processed 500/2388 title embeddings.\n","Processed 600/2388 title embeddings.\n","Processed 700/2388 title embeddings.\n","Processed 800/2388 title embeddings.\n","Processed 900/2388 title embeddings.\n","Processed 1000/2388 title embeddings.\n","Processed 1100/2388 title embeddings.\n","Processed 1200/2388 title embeddings.\n","Processed 1300/2388 title embeddings.\n","Processed 1400/2388 title embeddings.\n","Processed 1500/2388 title embeddings.\n","Processed 1600/2388 title embeddings.\n","Processed 1700/2388 title embeddings.\n","Processed 1800/2388 title embeddings.\n","Processed 1900/2388 title embeddings.\n","Processed 2000/2388 title embeddings.\n","Processed 2100/2388 title embeddings.\n","Processed 2200/2388 title embeddings.\n","Processed 2300/2388 title embeddings.\n","Processed 2388/2388 title embeddings.\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'nt' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-1ff170f5d024>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Save the updated DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab_Notebooks/data/full_data_with_finetuned_embeddings.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mnt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n🎉 All title Normalembeddings extracted successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Data saved at: /content/drive/My Drive/Colab_Notebooks/data/full_data_with_finetuned_embeddings.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nt' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8IS5_VpDvpoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHG8um7j57Oi","executionInfo":{"status":"ok","timestamp":1740483688567,"user_tz":-120,"elapsed":8102,"user":{"displayName":"עמרי ליברטי","userId":"17630927866470282449"}},"outputId":"9d0194e7-9cbd-4f6b-fb6d-19d1c3951ed3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings normalized and saved successfully!\n"]}]},{"cell_type":"markdown","source":["finetuned embedding for titles"],"metadata":{"id":"aBtfRtZgWyKY"}},{"cell_type":"code","source":[],"metadata":{"id":"RNQ3ITOZgJTu"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOIcewLklSgW0N7cH8ZjTcy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}